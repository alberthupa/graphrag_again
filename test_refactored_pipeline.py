#!/usr/bin/env python3
"""
Test script to verify the refactored entity extraction pipeline works correctly.

This script creates a mock log file to test the triplet generator without needing
to run the full extraction pipeline with API calls.
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))


def create_mock_extraction_log(log_file: str = "test_extraction_data.jsonl"):
    """Create a mock extraction log file for testing."""
    
    # Mock data similar to what run_extraction.py would generate
    mock_entries = [
        {
            "chunk_info": {
                "chunk_id": "chunk_001",
                "chunk_text": "Revenue is a key performance indicator (KPI) that measures total income generated by sales.",
                "chunk_metadata": {},
                "processed_at": datetime.now().isoformat(),
            },
            "entities": [
                {
                    "id": "entity_001",
                    "type": "KPI",
                    "name": "Revenue",
                    "description": "Total income generated by sales",
                    "confidence": 0.95,
                    "attributes": {"category": "financial"},
                    "source_chunk_id": "chunk_001",
                },
                {
                    "id": "entity_002", 
                    "type": "Definition",
                    "name": "Total income from sales",
                    "description": "A definition of revenue",
                    "confidence": 0.85,
                    "attributes": {},
                    "source_chunk_id": "chunk_001",
                }
            ],
            "relationships": [
                {
                    "id": "rel_001",
                    "subject_id": "entity_001",
                    "predicate": "hasDefinition",
                    "object_id": "entity_002",
                    "confidence": 0.90,
                    "context": "Revenue is defined as total income generated by sales",
                    "source_chunk_id": "chunk_001",
                }
            ],
        },
        {
            "chunk_info": {
                "chunk_id": "chunk_002", 
                "chunk_text": "Conversion rate measures the percentage of visitors who complete a desired action.",
                "chunk_metadata": {},
                "processed_at": datetime.now().isoformat(),
            },
            "entities": [
                {
                    "id": "entity_003",
                    "type": "KPI",
                    "name": "Conversion Rate",
                    "description": "Percentage of visitors who complete desired action",
                    "confidence": 0.92,
                    "attributes": {"category": "marketing"},
                    "source_chunk_id": "chunk_002",
                },
                {
                    "id": "entity_004",
                    "type": "Metric",
                    "name": "Percentage calculation",
                    "description": "Mathematical formula for percentage",
                    "confidence": 0.75,
                    "attributes": {},
                    "source_chunk_id": "chunk_002",
                }
            ],
            "relationships": [
                {
                    "id": "rel_002",
                    "subject_id": "entity_003",
                    "predicate": "calculatedBy",
                    "object_id": "entity_004",
                    "confidence": 0.88,
                    "context": "Conversion rate is calculated as a percentage",
                    "source_chunk_id": "chunk_002",
                }
            ],
        }
    ]
    
    # Write mock data to log file
    with open(log_file, "w", encoding="utf-8") as f:
        for entry in mock_entries:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    
    print(f"âœ“ Created mock extraction log: {log_file}")
    return log_file


def test_refactored_pipeline():
    """Test the refactored pipeline with mock data."""
    print("ğŸ§ª Testing Refactored Entity Extraction Pipeline")
    print("=" * 60)
    
    # Create mock log file
    log_file = create_mock_extraction_log()
    
    try:
        # Test triplet generator imports
        print("\nğŸ“¦ Step 1: Testing triplet generator imports...")
        
        from entity_extraction.triplet_generator import (
            load_extraction_data_from_log, 
            save_triplet_results,
            print_triplet_summary
        )
        print("âœ“ Triplet generator standalone imports successful")
        
        # Test loading extraction data from log
        print("\nğŸ“– Step 2: Testing extraction data loading from log...")
        extraction_result, source_text_map = load_extraction_data_from_log(log_file, verbose=False)
        
        print(f"âœ“ Loaded {len(extraction_result.entities)} entities")
        print(f"âœ“ Loaded {len(extraction_result.relationships)} relationships")
        print(f"âœ“ Source text map contains {len(source_text_map)} chunks")
        print(f"âœ“ Processed {extraction_result.total_chunks_processed} chunks")
        
        # Verify entity reconstruction
        if extraction_result.entities:
            sample_entity = extraction_result.entities[0]
            print(f"âœ“ Sample entity: {sample_entity.name} (type: {sample_entity.type.value})")
        
        # Verify relationship reconstruction  
        if extraction_result.relationships:
            sample_rel = extraction_result.relationships[0]
            print(f"âœ“ Sample relationship: {sample_rel.predicate.value}")
        
        # Test triplet generation
        print("\nğŸ”— Step 3: Testing triplet generation...")
        
        from triplet_generator_class_something import TripletGenerator
        triplet_generator = TripletGenerator(min_confidence=0.3)
        
        # Generate regular triplets
        triplets = triplet_generator.generate_triplets(extraction_result, source_text_map)
        print(f"âœ“ Generated {len(triplets)} regular triplets")
        
        # Generate KPI-focused triplets
        kpi_triplets = triplet_generator.generate_kpi_focused_triplets(extraction_result)
        print(f"âœ“ Generated {len(kpi_triplets)} KPI-focused triplets")
        
        # Combine triplets (avoiding duplicates)
        existing_triplet_ids = {t.id for t in triplets}
        for kpi_triplet in kpi_triplets:
            if kpi_triplet.id not in existing_triplet_ids:
                triplets.append(kpi_triplet)
        
        extraction_result.triplets = triplets
        print(f"âœ“ Total triplets: {len(triplets)}")
        
        # Verify triplet content
        if triplets:
            sample_triplet = triplets[0]
            print(f"âœ“ Sample triplet: {sample_triplet.subject.name} --[{sample_triplet.predicate.value}]--> {sample_triplet.object.name}")
            print(f"âœ“ Triplet confidence: {sample_triplet.confidence:.3f}")
        
        # Test saving results
        print("\nğŸ’¾ Step 4: Testing result saving...")
        
        triplets_summary = triplet_generator.export_triplets_summary(triplets)
        output_file = "test_triplet_results.json"
        
        save_triplet_results(extraction_result, triplets_summary, output_file, verbose=False)
        print(f"âœ“ Results saved to {output_file}")
        
        # Verify saved file
        if os.path.exists(output_file):
            with open(output_file, 'r') as f:
                saved_data = json.load(f)
            print(f"âœ“ Verified saved file contains {len(saved_data.get('triplets', []))} triplets")
        
        # Test summary generation
        print("\nğŸ“Š Step 5: Testing summary generation...")
        print_triplet_summary(extraction_result, triplets_summary, verbose=True)
        
        print(f"\nğŸ‰ Refactored pipeline integration test PASSED!")
        print(f"âœ“ Mock data successfully processed through both scripts")
        print(f"âœ“ Log-based data transfer works correctly")
        print(f"âœ“ Triplet generation works with reconstructed data")
        
    except Exception as e:
        print(f"\nâŒ Integration test FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        # Clean up mock files
        for file in [log_file, "test_triplet_results.json"]:
            if os.path.exists(file):
                os.remove(file)
                print(f"âœ“ Cleaned up: {file}")
    
    return True


def test_script_interfaces():
    """Test that both scripts have proper CLI interfaces."""
    print("\nğŸ–¥ï¸ Testing Script Interfaces")
    print("-" * 35)
    
    import subprocess
    
    try:
        # Test run_extraction.py help
        result1 = subprocess.run([
            "uv", "run", "entity_extraction/run_extraction.py", "--help"
        ], capture_output=True, text=True)
        
        if result1.returncode == 0 and "Extract entities from text chunks" in result1.stdout:
            print("âœ“ run_extraction.py CLI interface working")
        else:
            print("âŒ run_extraction.py CLI interface failed")
            return False
        
        # Test triplet_generator.py help
        result2 = subprocess.run([
            "uv", "run", "entity_extraction/triplet_generator.py", "--help"
        ], capture_output=True, text=True)
        
        if result2.returncode == 0 and "Generate triplets from extraction log data" in result2.stdout:
            print("âœ“ triplet_generator.py CLI interface working")
        else:
            print("âŒ triplet_generator.py CLI interface failed")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ CLI interface test failed: {e}")
        return False


def main():
    """Run all tests."""
    print("Testing Refactored Entity Extraction Pipeline")
    print("=" * 60)
    
    success = True
    
    try:
        # Test script interfaces first
        if not test_script_interfaces():
            success = False
        
        # Test the full pipeline integration
        if not test_refactored_pipeline():
            success = False
        
        if success:
            print("\nğŸ‰ All tests PASSED!")
            print("\nğŸ“ Usage Instructions:")
            print("1. Set OPENAI_API_KEY environment variable")
            print("2. Run: uv run entity_extraction/run_extraction.py")
            print("   â†’ Creates extraction_data.jsonl (comprehensive log) and extraction_results.json")
            print("3. Run: uv run entity_extraction/triplet_generator.py")
            print("   â†’ This reads extraction_data.jsonl and creates triplet_results.json")
            print("\nâœ¨ Benefits of refactored approach:")
            print("â€¢ Separation of concerns (extraction vs triplet generation)")
            print("â€¢ Can rerun triplet generation with different parameters")
            print("â€¢ Easier debugging and inspection of intermediate results")
            print("â€¢ More modular and maintainable code")
        else:
            print("\nâŒ Some tests FAILED!")
        
    except Exception as e:
        print(f"\nğŸ’¥ Test suite failed: {e}")
        import traceback
        traceback.print_exc()
        success = False

    return success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)